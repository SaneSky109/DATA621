---
title: 'DATA621 Final Project: Predicting Customer Churn'
author: "Eric Lehmphul"
date: "5/14/2022"
output: pdf_document
---


```{r, echo=F, warning=F, message=F}
library(tidyverse)
library(skimr)
library(corrplot)
library(caret)
library(ggpubr)
library(ROSE)
library(e1071)
library(modEvA)
library(ROCR)
library(PRROC)
library(pROC)
library(car)
```


# Abstract

Customer churn is problematic for businesses as they lose out on revenue every time a customer abandons the business. The objective of this paper is to create classification models to predict customer churn. Many classification models have been applied to churn detection in the past. The models used in this paper are binary logistic regression and naive bayes classifier. Most customer churn datasets contain data imbalance in the target variable. The dataset used in this paper is no different. External data balancing techniques were applied to the dataset and tested through the machine learning models allowing for the comparison of the data balancing techniques. The logistic regression model performed best using undersampled data, whereas the naive bayes model performed best using the oversampled data. The ability to predict customer churn using the unbalanced data before undergoing undersampling or oversampling yielded the worst results. The best model was Model 3 - "Logistic Regression with Undersampled Data", which was able to achieve a precision score of 0.782 maintained a fairly high recall and f1 score.

**Keywords:** customer churn, class imbalance, logistic regression, naive bayes, classification

# Introduction

Businesses rely of customers to provide the revenues needed to achieve profitability. Typically, it is more expensive to acquire a new customer than to retain an existing customer. According to an article in the *Harvard Business Review* written by Amy Gallo, it is "anywhere from 5 to 25 times more expensive" attracting new customers than retaining loyal customers. Customer churn is a real concern for almost all businesses, as losing customers is directly related to higher customer acquisition costs and loss in revenue. The rise of big data collection by companies allows the use of demographic, account, service, and activity information to be leveraged to create predictive models that determine the likelihood of a customer churning. 

The objective of this project is to predict whether a customer will change telecommunication service providers given information about the customerâ€™s engagement, telephone plan, and location. The dataset of interest contains information about customers of an undisclosed telecommunication provider and was obtained from Kaggle.com via the "Customer Churn Prediction 2020" contest. The motivation for conducting a detailed analysis on customer churn prediction is from the ability to genrate important business insights that will aid businesses in improving performance. 


# Literature Review

Customer churn is well known for having an unbalanced class distribution. Most customers are loyal to the business with only a percentage of customers churning. Data imbalance, huge volumes, and high dimensionality in telecommuncation data makes it arduous to draw meaningful and actionable insights (Eria & Marikannan, 2018). There are three significant developing techniques that have been applied to balancing imbalanced classes in literature related to customer churn: External, Algorithmic / internal, and Cost-sensitive (Ali et al., 2019).  The external approach focuses on rebalancing the data through modifying the dataset rather than adjusting the learning method of the machine learning model (Davarynejad, 2017). Internal approaches adjust or create algorithms to be able to handle imbalanced classification problems, such as modifying the decision threshold or creating a new cost function to add bias toward the minority class (Davarynejad, 2017). The last technique combines both the external and internal approaches (Davarynejad, 2017).

As customer churn is a classification problem, there are many available modeling techniques that one could use. Dahiya and Bhatia implemented decision tree and logistic regression models to predict churn and found decision trees to be more effective (Dahiya & Bhatia, 2015). Almana et al. used a neural networks, statistical models, decision trees, and covering algorithms for predicting churn (Almana et al., 2014). Yi Fei et al. applied Naive Bayes Classifier and K-means to detect customer churn (Yi Fei et al., 2017). These are only a handful of examples of different modeling techniques used in previous literature related to classifying customer churn. This shows that there are many different ways to approach a classification based churn problem.



# Methodology

The dataset is composed of a variety of metadata related to the customer including activity levels and geographic location (can be found at https://www.kaggle.com/competitions/customer-churn-prediction-2020/data?select=train.csv). The target variable, `churn`, is a binary indicator variable representing if a customer left or stayed. As mentioned in the Literature Review section, customer churn data is highly unbalanced and should be adjusted to conduct less bias models. External data balancing techniques were used to prepare the data for modeling. This study utilized both undersampling and oversampling data balancing approaches and compared the modeling results between the two methods. Binary logistic regression and naive bayes classifier models were used to predict telecommunication customer churn. The main metrics for determining the best model include precision, recall, f1 score. Precision will be the most important factor as the goal of the project is to predict customer churn which is a rare event. If accuracy is used, a model that only predicts that a customer will stay will yield a high accuracy, but is terrible at detecting customer churn. This is why precision is the preferred metric. 



# Experimentation and Results



## Data Descriptions

| Variable          | Data Type | Description                                                                                                                                                                         
|----------------|---------|-------------------------
| state             | String | 2-letter code of the US state of customer residence                                                                                             
| account_length              | Numeric | Number of months the customer has been with the current telco provider                                                                                                                                                              
| area_code              | String | 3 digit area code of customer                                                                                                     
| international_plan               | String | Indicator variable to identify if customer has an international plan                                                                                                      
| voice_mail_plan             | String | Indicator variable to identify if customer has a voice mail plan      
| number_vmail_messages            | Numeric | Number of voice mail messages recieved by customer 	
| total_day_minutes  | Numeric | Total minutes of day calls                                                                                                                      
| total_day_calls           	|  Numeric |Total number of day calls                                                                                                                       
| total_day_charge     | Numeric | Total charge of day calls                                                                                                                      
| total_eve_minutes              | Numeric | Total minutes of evening calls                                                                                                          
| total_eve_calls           	| Numeric | Total number of evening calls                                                                                                                       
| total_eve_charge    | Numeric | Total charge of evening calls                                                                                                                         
| total_night_minutes            | Numeric | Total minutes of night calls
| total_night_calls             | Numeric | Total number of night calls                                                                                            
| total_night_charge              | Numeric | Total charge of night calls                                                                                                                                                              
| total_intl_minutes              | Numeric | Total minutes of international calls                                                                                                     
| total_intl_calls               | Numeric | Total number of international calls                                                                                                     
| total_intl_charge             | Numeric | Total charge of international calls      
| number_customer_service_calls            | Numeric | Number of calls to customer service 	
| churn  | String | Binary indicator variable to identify if customer churned                                 

```{r,echo=FALSE}
churn.data <-  read.csv("https://raw.githubusercontent.com/SaneSky109/DATA621/main/Final_Project/Data/train.csv")
```

```{r,echo=FALSE}
churn.data$area_code <- str_remove_all(churn.data$area_code, "area_code_")
```

```{r,echo=FALSE}
churn.data$state <- as.factor(churn.data$state)
churn.data$area_code <- as.factor(churn.data$area_code)
churn.data$international_plan <- as.factor(churn.data$international_plan)
churn.data$voice_mail_plan <- as.factor(churn.data$voice_mail_plan)
churn.data$churn <- as.factor(churn.data$churn)
```



## Data Exploration


### Data Summary

There are no missing values present in this data source. Most of the numeric data are counts of customer activity, such as the number of phone calls made and the and the number of minutes used. Other variables pertain to metadata related to the customer, like state and area code.  

```{r,echo=FALSE}
summary_table <- skim_with(numeric = sfl(median = ~ median(., na.rm = TRUE),
                                         min = ~ min(., na.rm = TRUE),
                                         max = ~ max(., na.rm = TRUE),
                                         hist = NULL, p0 = NULL, p25 = NULL,
                                         p50 = NULL, p75 = NULL, p100 = NULL))

summary_table(churn.data)
```




### Distribution for Numeric Variables

Most of the variables appear to follow a near normal data distribution. The variables `number_customer_service_calls` and `number_vmail_messages` are count variables with right skewed distributions.

```{r,echo=FALSE}
churn.data %>% 
  gather(-c(state, area_code, international_plan, voice_mail_plan, churn), key = variable, value = value) %>%
  ggplot(., aes(x = value)) +
  geom_histogram(aes(x=value, y = ..density..), bins = 30, fill="#69b3a2", color="#e9ecef") +
  geom_density(aes(x=value), color='red', lwd = 1.75) +
  facet_wrap(~variable, scales ="free", ncol = 4)
```

### Check for Data Imbalance in Target Variable - churn

The target variable is dominated by the "no" class, indicating that customer churn is a rare event to occur.
```{r,echo=FALSE}
churn.data %>%
  ggplot(aes(x=churn)) + geom_histogram(stat="count",fill="black") +
  ggtitle("Churn - Data Distribution")
```


### Correlation Matrix

Most of the variables are not highly correlated with eachother. As expected, the variables that are highly correlated are the number of phone calls and the total amount charged to the customer.


```{r,echo=FALSE}
num.data <- churn.data[,-c(1,3,4,5,20)]
corrplot(cor(num.data), method = 'shade', order = 'AOE',col= colorRampPalette(c("red","tan", "blue"))(10) , diag = FALSE)
```

## Data Preprocessing

### Adjusted Data Types

Five of the variables were stored in the incorrect data type. The most common error was that the categorical variables were stored as a string data type instead of a factor data type. Below is a list of the adjusted variables.

* String to Factor Data Type
  - `state`
  - `area_code`
  - `international_plan`
  - `voice_mail_plan`
  - `churn`

### Modified Area Code Variable

The variable, `area_code`, was stored as 'area_code_###' where ### is a 3 digit area code. I modified this variable to only include the 3 digit number by removing 'area_code_' in all rows.

### Create Balanced Dataset for modeling

As shown in the Data Exploration, `churn` has a large data imbalance. This tends to be problematic for building effective classification models if left unaccounted for. I employed both undersampling and oversampling techniques to be used on the training dataset to build classification models. With imbalanced data, accuracy is not the best metric to use. I will be using F1 score as the main method of model evaluation.

```{r,echo=FALSE}
set.seed(15)

trainIndex <- createDataPartition(churn.data$churn, p = .7,
                                  list = FALSE,
                                  times = 1)

train <- churn.data[ trainIndex,]
test <- churn.data[-trainIndex,]
```

```{r,echo=FALSE}
over <- ovun.sample(churn~., data = train, method = "over")$data

under <- ovun.sample(churn~., data=train, method = "under")$data
```

```{r,echo=FALSE}
p1 <- over %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#7DCEA0") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Oversampled Data")

p2 <- under %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#D98880") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Undersampled Data")




ggarrange(p1,p2,
          ncol = 2, nrow = 1)
```

## Modeling

Six models were built to produce an effective classifier of customer churn in the telecommunication sector. The first three models are created using a logistic regression algorithm and the last three models use a naive bayes classifier to predict customer churn. To keep the models consistent, both algorithms were trained with the same three training datasets (Original training data, Oversampled data, and undersampled data). The Logistic regression and naive bayes classifier use the same dependent variables for the corresponding data source. For example, Model 1 and Model 4 are both using the original training data, so the model formula will be identical. The models are using different datasets and algorithms from one another, hindering the use of AIC, BIC, etc. to assess model performance, Instead, I will use metrics obtained by the confusion matrices and ROC curves to compare model performance. The following models were explored in this experimentation:


  * Model 1: Unbalanced Data Logistic Regression
  * Model 2: Oversampled Data Logistic Regression
  * Model 3: Undersampled Data Logistic Regression
  * Model 4: Unbalanced Data Naive Bayes
  * Model 5: Oversampled Data Naive Bayes
  * Model 6: Undersampled Data Naive Bayes

### Train Test Split

The customer churn dataset was divided into a train and test set to be able to assess model performance. The training dataset represents 70% of the customer churn data and the test contains the remaining 30%. Stratified random sampling was used to obtain the train and test datasets as it is important to guarantee that the target variable `churn` is represented equally in both datasets.  


```{r,echo=FALSE}
p1 <- train %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#5DADE2") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Training Data - Churn Distribution")

p2 <- test %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#EB984E") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Testing Data - Churn Distribution")


ggarrange(p1,p2,
          ncol = 2, nrow = 1)
```

### Logistic Regression Models

#### Model 1 - Unbalanced Data Logistic Regression


```{r,echo=FALSE}
all.variables <- glm(churn ~ .-total_day_charge, family = "binomial", data = train)

m1 <- step(all.variables, direction = "backward", trace = 0)

summary(m1)
```

##### Variance Inflation Factor

```{r,echo=FALSE}
vif(m1)
```

##### Confusion Matrix and ROC Curves

```{r,echo=FALSE}
pred.1.raw <- predict(m1, type = "response", newdata = test)
pred.1 <- as.factor(ifelse(pred.1.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.1)

cm1 <- confusionMatrix(cm, positive="yes")

cm1
```



```{r,echo=FALSE}
plot.rocs1 <- function(model, model_num){
  par(mfrow=c(1,2))
  prednb <- predict(model, test, type='response')

pred<-prediction(prednb, test$churn, label.ordering = c("no", "yes"))
perf<-performance(pred,"tpr","fpr")
p1 <- plot(perf, main=paste0("Model ", model_num, ": ROC curve"))
abline(0,1)
auc <- performance(pred,"auc")
auc.val <- auc@y.values[[1]]
text(x = 0.5, y = 0.25, labels = paste0("AUC: ", round(auc.val,3)),
    cex = 1)

perf1 <- performance(pred, "prec", "rec")
aucpr<- performance(pred,"aucpr")
aucpr.val<- aucpr@y.values[[1]]
p2 <- plot(perf1, main=paste0("Model ", model_num, ": Precision-recall curve"))
abline(1,-1)
text(x = 0.5, y = 0.25, labels = paste0("AUC PR: ", round(aucpr.val,3)),
    cex = 1)
}
```


```{r,echo=FALSE}
plot.rocs1(m1,1)
```

#### Model 2 - Oversampled Data

```{r,echo=FALSE}
all.variables <- glm(churn ~ .-total_day_minutes-total_eve_charge, family = "binomial", data = over)

m2 <- step(all.variables, direction = "backward", trace = 0)

summary(m2)
```

##### Variance Inflation Factor

```{r,echo=FALSE}
vif(m2)
```

##### Confusion Matrix and ROC Curves

```{r,echo=FALSE}
pred.2.raw <- predict(m2, type = "response", newdata = test)
pred.2 <- as.factor(ifelse(pred.2.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.2)

cm2 <- confusionMatrix(cm, positive="yes")

cm2
```

```{r,echo=FALSE}
plot.rocs1(m2,2)
```

#### Model 3 - Undersampled Data

```{r,echo=FALSE}
all.variables <- glm(churn ~ .-total_day_minutes, family = "binomial", data = under)

m3 <- step(all.variables, direction = "backward", trace = 0)

summary(m3)
```


##### Variance Inflation Factor

```{r,echo=FALSE}
vif(m3)
```


##### Confusion Matrix and ROC Curves

```{r,echo=FALSE}
pred.3.raw <- predict(m3, type = "response", newdata = test)
pred.3 <- as.factor(ifelse(pred.3.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.3)

cm3 <- confusionMatrix(cm, positive="yes")

cm3
```

```{r,echo=FALSE}
plot.rocs1(m3,3)
```

### Naive Bayes Classifier

#### Model 4 - Unbalanced Data


##### Confusion Matrix and ROC Curves


```{r,echo=FALSE}
m4 <- naiveBayes(churn ~ international_plan + voice_mail_plan + 
    total_day_minutes + total_eve_minutes + total_night_minutes + 
    total_intl_minutes + total_intl_calls + number_customer_service_calls, data = train)

y_pred <- predict(m4, type = "class", newdata = test)
cm <- table(test$churn, y_pred)

cm4 <- confusionMatrix(cm, positive="yes")

cm4
```




```{r,echo=FALSE}
plot.rocs <- function(model, model_num){
  par(mfrow=c(1,2))
  prednb <- predict(model, test, type='raw')

pred<-prediction(prednb[,2], test$churn, label.ordering = c("no", "yes"))
perf<-performance(pred,"tpr","fpr")
p1 <- plot(perf, main=paste0("Model ", model_num, ": ROC curve"))
abline(0,1)
auc <- performance(pred,"auc")
auc.val <- auc@y.values[[1]]
text(x = 0.5, y = 0.25, labels = paste0("AUC: ", round(auc.val,3)),
    cex = 1)

perf1 <- performance(pred, "prec", "rec")
aucpr<- performance(pred,"aucpr")
aucpr.val<- aucpr@y.values[[1]]
p2 <- plot(perf1, main=paste0("Model ", model_num, ": Precision-recall curve"))
abline(1,-1)
text(x = 0.5, y = 0.25, labels = paste0("AUC PR: ", round(aucpr.val,3)),
    cex = 1)

p1
p2
}

plot.rocs(m4,4)
```

#### Model 5 - Oversampled Data 

##### Confusion Matrix and ROC Curves

```{r,echo=FALSE}
m5 <- naiveBayes(churn ~ state + international_plan + voice_mail_plan + 
    number_vmail_messages + total_day_calls + total_day_charge + 
    total_eve_minutes + total_eve_calls + total_night_minutes + 
    total_night_calls + total_intl_minutes + total_intl_calls + 
    number_customer_service_calls, data = over)

y_pred <- predict(m5, newdata = test)
cm <- table(test$churn, y_pred)

cm5 <- confusionMatrix(cm, positive="yes")

cm5
```



```{r,echo=FALSE}
plot.rocs(m5,5)
```

#### Model 6 - Undersampled Data 

##### Confusion Matrix and ROC Curves

```{r,echo=FALSE}
m6 <- naiveBayes(churn ~ international_plan + voice_mail_plan + 
    number_vmail_messages + total_day_charge + total_eve_charge + 
    total_intl_charge + number_customer_service_calls, data = under)

y_pred <- predict(m6, newdata = test)
cm <- table(test$churn, y_pred)

cm6 <- confusionMatrix(cm, positive="yes")

cm6
```

```{r,echo=FALSE}
plot.rocs(m6,6)
```





## Evaluate Models

```{r,echo=FALSE}
get_metrics <- function(confusion.matrix){
by.class <- round(confusion.matrix$byClass, 3)
metrics <- data.frame(t(by.class))

Accuracy <- round(confusion.matrix$overall[1], 3)

classification.metrics <- cbind(Accuracy, metrics)

return(classification.metrics)
}
```

```{r,echo=FALSE}
metric1 <- get_metrics(cm1)
metric2 <- get_metrics(cm2)
metric3 <- get_metrics(cm3)
metric4 <- get_metrics(cm4)
metric5 <- get_metrics(cm5)
metric6 <- get_metrics(cm6)

model.metrics <- rbind(metric1, metric2, metric3, metric4, metric5, metric6)
rownames(model.metrics) <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6")

AUC <- c(0.812, 0.816, 0.814, 0.862, 0.799, 0.803)
AUC_PR <- c(0.476, 0.461, 0.45, 0.538, 0.505, 0.499)

model.metrics <- t(model.metrics)

model.metrics <- rbind(model.metrics, AUC, AUC_PR)


kableExtra::kable(model.metrics)
```

### Results 

The logistic regression and naive bayes models performed similarly across the 6 models. Logistic Regression outperformed the naive bayes classifier when using undersampled data (Model 3 vs Model 6) in terms of precision, recall, f1 score, and detection rate. The Naive Bayes classifier was a better model when using oversampled data (Model 2 vs Model 5) because it had a higher precision, recall, f1 score, and detection rate. The best model is Model 3 - Logistic Regression with undersampled data because it yeilds the highest precision in classifying that a customer will churn. 


# Discussion and Conclusion

The resulting models created in this study allow for customer churn to be detected with high precision. Given the same input features, a telecommunication company can detect potential customers of interest that are likely to leave. The telecommunication company can choose to either accept that the consumer will churn or they can take steps to retain the customer for continued service.

This study compared the techniques, undersampling and oversampling, for handling imbalanced data. Logistic regression was the best modeling algorithm when using undersampling techniques and naive bayes classifier was  the best modeling algorithm when using oversampled data. The best results across all models was Model 3 - "Logistic Regression with Undersampled Data", which was able to achieve a precision score of 0.782. It should also be noted that the training dataset without any target class balancing techniques yielded the worst results for detecting customer churn.




# References

Ali, H., Nahib Mohd Salleh, M., Saedudin, R., Hussain, K., & Mushtaq, M. F. (2019). Imbalance class problems in data mining: a review. Indonesian Journal of Electrical Engineering and Computer Science, 14(3), 1560â€“1571. https://doi.org/10.11591/ijeecs.v14.i3.pp1560-1571 


Almana, A. M., Aksoy, M. S., & Alzahran, R. (2014). A Survey On Data Mining Techniques In Customer Churn Analysis For Telecom Industry. Int. Journal of Engineering Research and Applications, 4(5), 165â€“171. Retrieved from https://www.ijera.com/papers/Vol4_issue5/Version%206/AF4506165171.pdf. 


Dahiya, K., & Bhatia, S. (2015). Customer churn analysis in telecom industry. IEEE. Retrieved from https://ieeexplore.ieee.org/abstract/document/7359318/authors#authors. 


Davarynejad, M. (2017, July 29). Classification with imbalanced data: Internal and external approaches and selection performance metrics. Dr. Ir. Mohsen Davarynejad. Retrieved May 21, 2022, from https://behsys.com/mohsen/Classification-with-Imbalanced-Data-Internal-External-Approaches-Selection-Performance-Metrics.html#internal-approaches 


Eria, K., & Marikannan, B. P. (2018). Systematic Review of Customer Churn Prediction in the Telecom. Journal of Applied Technology and Innovation, 2(1), 7â€“14. Retrieved from https://jati.sites.apiit.edu.my/files/2018/07/2018_Issue1_Paper2.pdf. 


Gallo, A. (2014, November 5). The value of keeping the right customers. Harvard Business Review. Retrieved May 21, 2022, from https://hbr.org/2014/10/the-value-of-keeping-the-right-customers 


Yi Fei, T., Hai Shuan, L., Jie Yan, L., Xiaoning, G., & Wooi King, S. (2017). Prediction on Customer Churn in the Telecommunications Sector Using Discretization and NaÃ¯ve Bayes Classifier. Int. Journal Advanced Software Computer. Applications, 9(3), 24â€“35. Retrieved from https://www.i-csrs.org/Volumes/ijasca/2_Page-23_35_Predictive-Analysis-for-Telecommunications-Customer-Churn-on-Big-Data-Platform.pdf. 


# Appendices

```{r,eval=FALSE}
library(tidyverse)
library(skimr)
library(corrplot)
library(caret)
library(ggpubr)
library(ROSE)
library(e1071)
library(modEvA)
library(ROCR)
library(PRROC)
library(pROC)
library(car)

churn.data <-  read.csv("https://raw.githubusercontent.com/SaneSky109/DATA621/main/Final_Project/Data/train.csv")

churn.data$area_code <- str_remove_all(churn.data$area_code, "area_code_")

churn.data$state <- as.factor(churn.data$state)
churn.data$area_code <- as.factor(churn.data$area_code)
churn.data$international_plan <- as.factor(churn.data$international_plan)
churn.data$voice_mail_plan <- as.factor(churn.data$voice_mail_plan)
churn.data$churn <- as.factor(churn.data$churn)

summary_table <- skim_with(numeric = sfl(median = ~ median(., na.rm = TRUE),
                                         min = ~ min(., na.rm = TRUE),
                                         max = ~ max(., na.rm = TRUE),
                                         hist = NULL, p0 = NULL, p25 = NULL,
                                         p50 = NULL, p75 = NULL, p100 = NULL))

summary_table(churn.data)

churn.data %>% 
  gather(-c(state, area_code, international_plan, voice_mail_plan, churn), key = variable, value = value) %>%
  ggplot(., aes(x = value)) +
  geom_histogram(aes(x=value, y = ..density..), bins = 30, fill="#69b3a2", color="#e9ecef") +
  geom_density(aes(x=value), color='red', lwd = 1.75) +
  facet_wrap(~variable, scales ="free", ncol = 4)

churn.data %>%
  ggplot(aes(x=churn)) + geom_histogram(stat="count",fill="black") +
  ggtitle("Churn - Data Distribution")

num.data <- churn.data[,-c(1,3,4,5,20)]
corrplot(cor(num.data), method = 'shade', order = 'AOE',col= colorRampPalette(c("red","tan", "blue"))(10) , diag = FALSE)

set.seed(15)

trainIndex <- createDataPartition(churn.data$churn, p = .7,
                                  list = FALSE,
                                  times = 1)

train <- churn.data[ trainIndex,]
test <- churn.data[-trainIndex,]

over <- ovun.sample(churn~., data = train, method = "over")$data

under <- ovun.sample(churn~., data=train, method = "under")$data

p1 <- over %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#7DCEA0") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Oversampled Data")

p2 <- under %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#D98880") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Undersampled Data")




ggarrange(p1,p2,
          ncol = 2, nrow = 1)

p1 <- train %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#5DADE2") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Training Data - Churn Distribution")

p2 <- test %>%
  ggplot(aes(x=churn)) + geom_bar(fill = "#EB984E") +
  geom_text(stat='count', aes(label=..count..), vjust=2) +
  ggtitle("Testing Data - Churn Distribution")


ggarrange(p1,p2,
          ncol = 2, nrow = 1)

all.variables <- glm(churn ~ .-total_day_charge, family = "binomial", data = train)

m1 <- step(all.variables, direction = "backward", trace = 0)

summary(m1)

vif(m1)

pred.1.raw <- predict(m1, type = "response", newdata = test)
pred.1 <- as.factor(ifelse(pred.1.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.1)

cm1 <- confusionMatrix(cm, positive="yes")

cm1

plot.rocs1 <- function(model, model_num){
  par(mfrow=c(1,2))
  prednb <- predict(model, test, type='response')

pred<-prediction(prednb, test$churn, label.ordering = c("no", "yes"))
perf<-performance(pred,"tpr","fpr")
p1 <- plot(perf, main=paste0("Model ", model_num, ": ROC curve"))
abline(0,1)
auc <- performance(pred,"auc")
auc.val <- auc@y.values[[1]]
text(x = 0.5, y = 0.25, labels = paste0("AUC: ", round(auc.val,3)),
    cex = 1)

perf1 <- performance(pred, "prec", "rec")
aucpr<- performance(pred,"aucpr")
aucpr.val<- aucpr@y.values[[1]]
p2 <- plot(perf1, main=paste0("Model ", model_num, ": Precision-recall curve"))
abline(1,-1)
text(x = 0.5, y = 0.25, labels = paste0("AUC PR: ", round(aucpr.val,3)),
    cex = 1)
}

plot.rocs1(m1,1)

all.variables <- glm(churn ~ .-total_day_minutes-total_eve_charge, family = "binomial", data = over)

m2 <- step(all.variables, direction = "backward", trace = 0)

summary(m2)

vif(m2)

pred.2.raw <- predict(m2, type = "response", newdata = test)
pred.2 <- as.factor(ifelse(pred.2.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.2)

cm2 <- confusionMatrix(cm, positive="yes")

cm2

plot.rocs1(m2,2)

all.variables <- glm(churn ~ .-total_day_minutes, family = "binomial", data = under)

m3 <- step(all.variables, direction = "backward", trace = 0)

summary(m3)

vif(m3)

pred.3.raw <- predict(m3, type = "response", newdata = test)
pred.3 <- as.factor(ifelse(pred.3.raw < .5, "no", "yes"))

cm <- table(test$churn, pred.3)

cm3 <- confusionMatrix(cm, positive="yes")

cm3

plot.rocs1(m3,3)

m4 <- naiveBayes(churn ~ international_plan + voice_mail_plan + 
    total_day_minutes + total_eve_minutes + total_night_minutes + 
    total_intl_minutes + total_intl_calls + number_customer_service_calls, data = train)

y_pred <- predict(m4, type = "class", newdata = test)
cm <- table(test$churn, y_pred)

cm4 <- confusionMatrix(cm, positive="yes")

cm4

plot.rocs <- function(model, model_num){
  par(mfrow=c(1,2))
  prednb <- predict(model, test, type='raw')

pred<-prediction(prednb[,2], test$churn, label.ordering = c("no", "yes"))
perf<-performance(pred,"tpr","fpr")
p1 <- plot(perf, main=paste0("Model ", model_num, ": ROC curve"))
abline(0,1)
auc <- performance(pred,"auc")
auc.val <- auc@y.values[[1]]
text(x = 0.5, y = 0.25, labels = paste0("AUC: ", round(auc.val,3)),
    cex = 1)

perf1 <- performance(pred, "prec", "rec")
aucpr<- performance(pred,"aucpr")
aucpr.val<- aucpr@y.values[[1]]
p2 <- plot(perf1, main=paste0("Model ", model_num, ": Precision-recall curve"))
abline(1,-1)
text(x = 0.5, y = 0.25, labels = paste0("AUC PR: ", round(aucpr.val,3)),
    cex = 1)

p1
p2
}

plot.rocs(m4,4)

m5 <- naiveBayes(churn ~ state + international_plan + voice_mail_plan + 
    number_vmail_messages + total_day_calls + total_day_charge + 
    total_eve_minutes + total_eve_calls + total_night_minutes + 
    total_night_calls + total_intl_minutes + total_intl_calls + 
    number_customer_service_calls, data = over)

y_pred <- predict(m5, newdata = test)
cm <- table(test$churn, y_pred)

cm5 <- confusionMatrix(cm, positive="yes")

cm5

plot.rocs(m5,5)

m6 <- naiveBayes(churn ~ international_plan + voice_mail_plan + 
    number_vmail_messages + total_day_charge + total_eve_charge + 
    total_intl_charge + number_customer_service_calls, data = under)

y_pred <- predict(m6, newdata = test)
cm <- table(test$churn, y_pred)

cm6 <- confusionMatrix(cm, positive="yes")

cm6

plot.rocs(m6,6)

get_metrics <- function(confusion.matrix){
by.class <- round(confusion.matrix$byClass, 3)
metrics <- data.frame(t(by.class))

Accuracy <- round(confusion.matrix$overall[1], 3)

classification.metrics <- cbind(Accuracy, metrics)

return(classification.metrics)
}

metric1 <- get_metrics(cm1)
metric2 <- get_metrics(cm2)
metric3 <- get_metrics(cm3)
metric4 <- get_metrics(cm4)
metric5 <- get_metrics(cm5)
metric6 <- get_metrics(cm6)

model.metrics <- rbind(metric1, metric2, metric3, metric4, metric5, metric6)
rownames(model.metrics) <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6")

AUC <- c(0.812, 0.816, 0.814, 0.862, 0.799, 0.803)
AUC_PR <- c(0.476, 0.461, 0.45, 0.538, 0.505, 0.499)

model.metrics <- t(model.metrics)

model.metrics <- rbind(model.metrics, AUC, AUC_PR)


kableExtra::kable(model.metrics)

```
